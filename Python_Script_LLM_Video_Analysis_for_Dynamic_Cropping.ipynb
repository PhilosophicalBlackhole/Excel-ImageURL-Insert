{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1j_4FEb3oSm7TrW15j-m36-ywonV77z_j",
      "authorship_tag": "ABX9TyP/VBJb38bhUekTYczKvacV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilosophicalBlackhole/Excel-ImageURL-Insert/blob/main/Python_Script_LLM_Video_Analysis_for_Dynamic_Cropping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38f7d5c7",
        "outputId": "eedf3f3a-49f2-4d64-96ea-244d418fd718"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-20 22:31:46--  https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8342 (8.1K) [text/plain]\n",
            "Saving to: ‘yolov3.cfg’\n",
            "\n",
            "yolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-20 22:31:46 (59.6 MB/s) - ‘yolov3.cfg’ saved [8342/8342]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79013805",
        "outputId": "2f43ec76-32c2-4712-963c-b153822d0121"
      },
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-20 22:31:57--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 172.67.185.199, 104.21.88.156, 2606:4700:3037::6815:589c, ...\n",
            "Connecting to pjreddie.com (pjreddie.com)|172.67.185.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights          [ <=>                ]   8.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-20 22:31:58 (57.7 MB/s) - ‘yolov3.weights’ saved [9093]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "102f3a8b",
        "outputId": "23ff20c1-e77f-42df-8b0c-6ba1906755cc"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-20 22:32:04--  https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 625 [text/plain]\n",
            "Saving to: ‘coco.names’\n",
            "\n",
            "coco.names          100%[===================>]     625  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-20 22:32:05 (28.4 MB/s) - ‘coco.names’ saved [625/625]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "54c01833",
        "outputId": "030348ce-0d31-487b-bbd0-77ec94928723"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "!pip install opencv-python\n",
        "# You will need to download yolov3.cfg, yolov3.weights, and coco.names separately\n",
        "# !wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
        "# !wget https://pjreddie.com/media/files/yolov3.weights\n",
        "# !wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\n",
        "\n",
        "# --- Model Configuration ---\n",
        "# Replace with the actual paths to your model configuration and weights files\n",
        "# You can download YOLO files from various sources, e.g., https://github.com/pjreddie/darknet\n",
        "config_path = \"yolov3.cfg\"\n",
        "weights_path = \"yolov3.weights\"\n",
        "labels_path = \"coco.names\" # Optional: file with class names\n",
        "\n",
        "# Load the DNN model\n",
        "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
        "\n",
        "# Get the output layer names\n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "# Load class names (if available)\n",
        "classes = None\n",
        "with open(labels_path, 'r') as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# --- Video Processing (Modified from your original code) ---\n",
        "cap = cv2.VideoCapture(\"input_video.mp4\")\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Initialize video writer for cropped output (adjust dimensions if needed)\n",
        "crop_width, crop_height = 400, 400  # example crop size around the subject\n",
        "out = cv2.VideoWriter(\"cropped_output.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (crop_width, crop_height))\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Prepare the frame for the model\n",
        "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outs = net.forward(output_layers)\n",
        "\n",
        "    # Process the model output (example: find a dominant object like a person or window)\n",
        "    class_ids = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > 0.5: # Confidence threshold\n",
        "                # Object detected\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "\n",
        "                # Rectangle coordinates\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "    # Apply Non-Maximum Suppression to remove redundant overlapping boxes\n",
        "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4) # Confidence and NMS thresholds\n",
        "\n",
        "    # Find the bounding box for the object you are interested in (e.g., a window)\n",
        "    # This part requires specific logic based on what you want to detect.\n",
        "    # For example, if you are looking for a \"window\" object and it's class ID 10:\n",
        "    target_bbox = None\n",
        "    if classes:\n",
        "        try:\n",
        "            window_class_id = classes.index(\"window\") # Or whatever your target object is\n",
        "            for i in indexes:\n",
        "                if class_ids[i] == window_class_id:\n",
        "                    target_bbox = boxes[i]\n",
        "                    break # Found the target object, take the first one\n",
        "\n",
        "        except ValueError:\n",
        "            print(\"Warning: 'window' class not found in coco.names. Adapt this part to your target object.\")\n",
        "            # As a fallback, you might just take the largest bounding box or the one with highest confidence\n",
        "            if len(indexes) > 0:\n",
        "                 best_index = indexes[0] # Example: take the first one after NMS\n",
        "                 target_bbox = boxes[best_index]\n",
        "\n",
        "\n",
        "    # If a target bounding box is found, crop the frame\n",
        "    if target_bbox is not None:\n",
        "        x = max(0, x)\n",
        "        y = max(0, y)\n",
        "        w = min(w, width - x)\n",
        "        h = min(h, height - y)\n",
        "\n",
        "        cropped_frame = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize cropped frame to uniform output size\n",
        "        if cropped_frame.shape[0] > 0 and cropped_frame.shape[1] > 0: # Check if crop is valid\n",
        "            cropped_frame = cv2.resize(cropped_frame, (crop_width, crop_height))\n",
        "            # Write to output video\n",
        "            out.write(cropped_frame)\n",
        "        else:\n",
        "             print(f\"Skipping frame due to invalid crop dimensions: {cropped_frame.shape}\")\n",
        "\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows() # Close any OpenCV windows"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv/modules/dnn/src/darknet/darknet_io.cpp:991: error: (-213:The function/feature is not implemented) Transpose the weights (except for convolutional) is not implemented in function 'ReadDarknetFromWeightsStream'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-934097157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Load the DNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNetFromDarknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Get the output layer names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/dnn/src/darknet/darknet_io.cpp:991: error: (-213:The function/feature is not implemented) Transpose the weights (except for convolutional) is not implemented in function 'ReadDarknetFromWeightsStream'\n"
          ]
        }
      ]
    }
  ]
}